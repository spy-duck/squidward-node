log_schema:
  level: "warn"

sources:
  squid_logs:
    type: "file"
    include:
      - "/var/log/squid/access.log"

transforms:
  # Parse logs
  parsed_log:
    type: "remap"
    inputs:
      - squid_logs
    source: |
      parsed, err = parse_regex(.message, r'^(?P<timestamp>[^ ]+)\s+(?P<user>[^ ]+)\s+(?P<client_ip>[^ ]+)\s+(?P<upload>[^ ]+)/(?P<download>[^ ]+)\s+(?P<method>[^ ]+)/(?P<http_status>[^ ]+)\s+(?P<url>[^ ]+)\s+(?P<request_scheme>[^ ]+)\s+(?P<content_type>[^ ]+)\s+(?P<request_status>[^ ]+)/(?P<hierarchy_status>[^ ]+)$')
      
      if err == null {
          .user = parsed.user
          .upload = to_int(parsed.upload) ?? 0
          .download = to_int(parsed.download) ?? 0
      
          del(.message)
          del(.timestamp)
          del(.file)
          del(.host)
          del(.client_ip)
          del(.method)
          del(.http_status)
          del(.request_scheme)
          del(.content_type)
          del(.request_status)
          del(.hierarchy_status)
      } else {
          abort
      }

  # Users stats
  filtered_log:
      type: "filter"
      inputs:
        - parsed_log
      condition: ".user != \"-\""

  aggregate_by_user:
    type: "reduce"
    inputs:
      - filtered_log
    group_by:
      - "user"
    merge_strategies:
      upload: "sum"
      download: "sum"
    flush_period_ms: ${VECTOR_LOGS_FLUSH_MS:-30000}

  users_remap:
    type: "remap"
    inputs:
      - aggregate_by_user
    source: |
      . = {
        "ts": to_unix_timestamp(now()),
        "usr": .user,
        "up": to_string!(.upload),
        "down":  to_string!(.download)
      }

  # Node stats
  aggregate_by_node:
    type: "reduce"
    inputs:
      - parsed_log
    group_by: []
    merge_strategies:
      upload: "sum"
      download: "sum"
    flush_period_ms: ${VECTOR_LOGS_FLUSH_MS:-30000}

  node_remap:
    type: "remap"
    inputs:
      - aggregate_by_node
    source: |
      . = {
        "ts": to_unix_timestamp(now()),
        "up": to_string!(.upload),
        "down":  to_string!(.download)
      }

  # Online users stats
  aggregate_online:
    type: "reduce"
    inputs:
      - filtered_log
    group_by:
      - "user"
    flush_period_ms: ${VECTOR_LOGS_FLUSH_MS:-5000}

  prep_online_count:
    type: "remap"
    inputs:
      - aggregate_online
    source: |
      .online = 1

  aggregate_by_user_online:
    type: "reduce"
    inputs:
      - prep_online_count
    group_by:
      - "user"
    merge_strategies:
      online: "sum"
    flush_period_ms: ${VECTOR_LOGS_FLUSH_MS:-5000}

  node_online_remap:
    type: "remap"
    inputs:
      - aggregate_by_user_online
    source: |
      . = {
        "ts": to_unix_timestamp(now()),
        "online": .online,
      }

sinks:
  redis_users_sink:
    type: "redis"
    inputs:
      - users_remap
    endpoint: "redis://:${REDIS_PASSWORD?err}@127.0.0.1:${REDIS_PORT:-6379}/${REDIS_DB:-0}"
    key: "metrics:users"
    data_type: "list"
    healthcheck:
      enabled: true
    encoding:
      codec: "json"
    buffer:
        type: "disk"
        max_size: 512000000 # 512MB

  redis_node_sink:
    type: "redis"
    inputs:
      - node_remap
    endpoint: "redis://:${REDIS_PASSWORD?err}@127.0.0.1:${REDIS_PORT:-6379}/${REDIS_DB:-0}"
    key: "metrics:node"
    data_type: "list"
    healthcheck:
      enabled: true
    encoding:
      codec: "json"
    buffer:
        type: "disk"
        max_size: 512000000 # 512MB

  redis_node_online_users_sink:
    type: "redis"
    inputs:
      - node_online_remap
    endpoint: "redis://:${REDIS_PASSWORD?err}@127.0.0.1:${REDIS_PORT:-6379}/${REDIS_DB:-0}"
    key: "metrics:online"
    data_type: "list"
    healthcheck:
      enabled: true
    encoding:
      codec: "json"
    buffer:
        type: "disk"
        max_size: 512000000 # 512MB